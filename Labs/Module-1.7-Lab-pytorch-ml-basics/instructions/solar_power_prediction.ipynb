{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solar-power-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrWpQNLOj8HS"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAOrT9OsjjGw"
      },
      "source": [
        "import math\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28copLJCleG6"
      },
      "source": [
        "torch.manual_seed(1)  # reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbAwKi54kMRJ"
      },
      "source": [
        "# Traning settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-JEfsRakK5u"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--data', default='data/solar_power.csv', type=str)\n",
        "parser.add_argument('--history', default=8, type=int, help='sequence history (in hours)')\n",
        "parser.add_argument('--horizon', default=1, type=int, help='forecasting horizon (in hours)')\n",
        "parser.add_argument('--split', default=0.8, type=float, help='train/test dataset split')\n",
        "parser.add_argument('--scaler', default='minmax', choices=['none', 'minmax', 'standard'], help='dataset preprocessing scaler to use')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.05, type=float, help='learning rate')\n",
        "parser.add_argument('--epochs', default=1000, type=int, help='number of training epochs')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "print(args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODoI2l5FmXCg"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUbOQFQJkSV1"
      },
      "source": [
        "print(f\"loading {args.data}\")\n",
        "df = pd.read_csv(args.data, parse_dates=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZdPSl0QkbyA"
      },
      "source": [
        "df = df[['AMBIENT_TEMPERATURE']]\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvHQ8d9fnL-I"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1dbIYdXnRgy"
      },
      "source": [
        "if args.scaler == 'minmax':\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "elif args.scaler == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "else:\n",
        "    scaler = None\n",
        "\n",
        "if scaler:\n",
        "    data = scaler.fit_transform(df.values)\n",
        "else:\n",
        "    data = df.values\n",
        "    \n",
        "print(data)\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqpOOLhPnT22"
      },
      "source": [
        "# Create Pytorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgBcTs6InSQN"
      },
      "source": [
        "def to_pytorch(array):\n",
        "    return torch.from_numpy(array).type(torch.FloatTensor).cuda()\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "    \n",
        "def unscale(array, resize=None):\n",
        "    if not scaler: return array\n",
        "    if len(array.shape) == 0: array = array.reshape(-1, 1)\n",
        "    if len(array.shape) == 1: array = np.expand_dims(array, 0)\n",
        "    #if resize: array = np.concatenate((np.zeros((resize[0], resize[1]-1)), array), axis=1)\n",
        "    array = scaler.inverse_transform(array)\n",
        "    #return array[:,-1] if resize else array\n",
        "    return array\n",
        "    \n",
        "def generate_sequences(data, sequence_length):\n",
        "    if sequence_length == 1:\n",
        "        return np.expand_dims(data,1)\n",
        "    seq = []\n",
        "    for index in range(len(data) - sequence_length): \n",
        "        seq.append(data[index : index + sequence_length]) \n",
        "    return np.array(seq)\n",
        "            \n",
        "def create_dataset(data, history, horizon):\n",
        "    # shift the data by the forecast length\n",
        "    x = data[:-horizon,:]\n",
        "    y = np.roll(data[:,-1],-horizon,axis=0)[:-horizon]\n",
        "    \n",
        "    # generate sequences\n",
        "    x = generate_sequences(x, history)\n",
        "    y = generate_sequences(y, history)[:,-1]\n",
        "    \n",
        "    # cast to pytorch tensors\n",
        "    x = to_pytorch(x)\n",
        "    y = to_pytorch(y).unsqueeze(dim=-1)\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "train_split = int(len(data) * args.split)\n",
        "\n",
        "x_train, y_train = create_dataset(data[:train_split,:], args.history, args.horizon)\n",
        "x_test, y_test = create_dataset(data[train_split:,:], args.history, args.horizon)\n",
        " \n",
        "print('x_train', x_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "\n",
        "print('x_test', x_test.shape)\n",
        "print('y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NWR5Oh5nhC7"
      },
      "source": [
        "# Create a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsBTxwBundzF"
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1, hidden_dim=32, num_layers=2):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.0)\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()\n",
        "        # UserWarning: RNN module weights are not part of single contiguous chunk of memory\n",
        "        #self.gru.flatten_parameters()   \n",
        "        x, (hn) = self.gru(x, (h0.detach()))\n",
        "        x = self.fc1(x[:, -1, :]) \n",
        "        return x\n",
        "\n",
        "net = GRU(x_train.shape[-1], 1).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-KJJ2dsnjz2"
      },
      "source": [
        "# Create loss function and solver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBIuKbBxnu_R"
      },
      "source": [
        "criterion = torch.nn.MSELoss().cuda()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)  \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 250, 0.5)\n",
        "\n",
        "def RMSE(y_pred, y):\n",
        "    return math.sqrt((np.square(unscale(to_numpy(y_pred)) - unscale(to_numpy(y)))).mean(axis=0).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8UQWny1nyVM"
      },
      "source": [
        "# Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5czItti0nw95"
      },
      "source": [
        "for epoch in range(args.epochs):\n",
        "    net.train()\n",
        "    \n",
        "    y_pred = net(x_train)\n",
        "    train_loss = criterion(y_pred, y_train)\n",
        "    train_rmse = RMSE(y_pred, y_train)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        y_pred = net(x_test)\n",
        "        test_loss = criterion(y_pred, y_test)\n",
        "        test_rmse = RMSE(y_pred, y_test)\n",
        "        #unscaled_loss = unscale(np.array(loss.item())).item()\n",
        "        #unscaled_test_loss = unscale(np.array(test_loss.item())).item()\n",
        "        print(f\"Epoch {epoch:03d}  LR={scheduler.get_last_lr()[0]}  train_loss={train_loss:.8f}  test_loss={test_loss:.8f}  train_rmse={train_rmse:.8f}  test_err={test_rmse:.8f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8xpcQooAAW"
      },
      "source": [
        "# Print out actual vs predicted values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAEl3b7mn6fk"
      },
      "source": [
        "# print out actual vs predicted values     \n",
        "#x_test = to_numpy(x_test)\n",
        "y_test = to_numpy(y_test)\n",
        "y_pred = to_numpy(y_pred)\n",
        " \n",
        "if scaler:\n",
        "    #x_test = unscale(x_test)\n",
        "    y_test = unscale(y_test, x_test.shape)\n",
        "    y_pred = unscale(y_pred, x_test.shape)\n",
        "\n",
        "print('')\n",
        "#print('x_test', x_test)\n",
        "print('y_test', y_test)\n",
        "print('y_pred', y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}